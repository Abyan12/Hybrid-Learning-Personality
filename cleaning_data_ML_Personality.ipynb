{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9RENHPssOc3cn3qmZjQSe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abyan12/Hybrid-Learning-Personality/blob/main/cleaning_data_ML_Personality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "BjkuAi1n2FeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    print(\"Data NLTK berhasil diunduh.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal mengunduh data NLTK: {e}\")\n"
      ],
      "metadata": {
        "id": "GqQU6k8d4v1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e807501-deca-40d9-b0d0-4508e907914b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data NLTK berhasil diunduh.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'reddit_personality_10k.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"\\nDataset berhasil dimuat. Berikut 5 baris pertama:\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nERROR: File '{file_path}' tidak ditemukan. Pastikan Anda sudah mengunggahnya ke Google Colab.\")\n",
        "    # Hentikan eksekusi jika file tidak ada\n",
        "    exit()"
      ],
      "metadata": {
        "id": "4Abg4N9R4xE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0bf34f-df48-48b5-9be0-b28167817888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset berhasil dimuat. Berikut 5 baris pertama:\n",
            "   subreddit                                              title  \\\n",
            "0  introvert                   Anyone else hybernate in summer?   \n",
            "1  introvert  Why I started treating social energy like a fi...   \n",
            "2  introvert           Do People Actually Like It When we Talk?   \n",
            "3  introvert          Anyone else find they don’t need friends?   \n",
            "4  introvert  Are there people who love going alone to conce...   \n",
            "\n",
            "                                             content      label  \\\n",
            "0  I honestly do not see the appeal of summer. I ...  introvert   \n",
            "1  After months of crashing from back-to-back mee...  introvert   \n",
            "2  It doesn't always feel like praise whenever an...  introvert   \n",
            "3  Sorry, mistake in the title. Had to repost. \\n...  introvert   \n",
            "4  I see many people being scared of going alone ...  introvert   \n",
            "\n",
            "           created_utc  \n",
            "0  2025-07-18 13:36:27  \n",
            "1  2025-07-18 14:01:34  \n",
            "2  2025-07-18 17:45:56  \n",
            "3  2025-07-18 16:33:28  \n",
            "4  2025-07-18 10:56:28  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'].fillna('', inplace=True)\n",
        "df['full_text'] = df['title'] + ' ' + df['content']\n",
        "print(\"\\nKolom 'title' dan 'content' berhasil digabung menjadi 'full_text'.\")"
      ],
      "metadata": {
        "id": "js9isnOk40em",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c3ba5c-d332-4c65-b4df-d6884da37c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Kolom 'title' dan 'content' berhasil digabung menjadi 'full_text'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1052583047.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['content'].fillna('', inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Fungsi untuk membersihkan teks dengan langkah-langkah berikut:\n",
        "    1. Menghapus URL\n",
        "    2. Menghapus mention user/subreddit\n",
        "    3. Menghapus tanda baca dan angka\n",
        "    4. Mengubah ke huruf kecil\n",
        "    5. Tokenisasi (memecah teks menjadi kata)\n",
        "    6. Menghapus stopwords\n",
        "    7. Lemmatisasi (mengubah kata ke bentuk dasar)\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Menghapus URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Menghapus mention user dan subreddit\n",
        "    text = re.sub(r'\\@\\w+|\\/r\\/\\w+|\\/u\\/\\w+', '', text)\n",
        "    # Menghapus semua karakter kecuali huruf dan spasi\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Mengubah ke huruf kecil\n",
        "    text = text.lower()\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text)\n",
        "    # Menghapus stopwords dan melakukan lemmatisasi\n",
        "    cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    # Menggabungkan kembali token menjadi teks\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "print(\"\\nFungsi preprocessing telah siap.\")"
      ],
      "metadata": {
        "id": "Zl3fKThW43f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc26e33e-8ddc-4759-99e4-9583c55e50ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fungsi preprocessing telah siap.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2067b77b",
        "outputId": "1a4768ba-4b3d-47f6-8809-63a2b42adb59"
      },
      "source": [
        "try:\n",
        "    nltk.download('punkt_tab')\n",
        "    print(\"Data NLTK punkt_tab berhasil diunduh.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal mengunduh data NLTK punkt_tab: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data NLTK punkt_tab berhasil diunduh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Memulai proses pembersihan teks...\")\n",
        "df['cleaned_text'] = df['full_text'].apply(preprocess_text)\n",
        "print(\"Pembersihan teks selesai!\")"
      ],
      "metadata": {
        "id": "tHxd1t-_47bN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2523d01-e538-45a8-875f-dc00f6ac5ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai proses pembersihan teks...\n",
            "Pembersihan teks selesai!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBerikut adalah perbandingan teks asli dengan teks yang sudah dibersihkan:\")\n",
        "print(df[['full_text', 'cleaned_text']].head())"
      ],
      "metadata": {
        "id": "J0iTTKSI5Lbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc373e4-4954-45fb-e900-13abfe6bee79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Berikut adalah perbandingan teks asli dengan teks yang sudah dibersihkan:\n",
            "                                           full_text  \\\n",
            "0  Anyone else hybernate in summer? I honestly do...   \n",
            "1  Why I started treating social energy like a fi...   \n",
            "2  Do People Actually Like It When we Talk? It do...   \n",
            "3  Anyone else find they don’t need friends? Sorr...   \n",
            "4  Are there people who love going alone to conce...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  anyone else hybernate summer honestly see appe...  \n",
            "1  started treating social energy like finite res...  \n",
            "2  people actually like talk doesnt always feel l...  \n",
            "3  anyone else find dont need friend sorry mistak...  \n",
            "4  people love going alone concert see many peopl...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_file_path = 'cleaned_reddit_personality.csv'\n",
        "# Memilih kolom yang relevan untuk disimpan\n",
        "df_to_save = df[['subreddit', 'label', 'created_utc', 'full_text', 'cleaned_text']]\n",
        "df_to_save.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "print(f\"\\nData yang sudah bersih berhasil disimpan ke file '{cleaned_file_path}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYcFeajn5QSW",
        "outputId": "75fae95c-114c-4f59-8bb2-fdf08f6dd70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data yang sudah bersih berhasil disimpan ke file 'cleaned_reddit_personality.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"Anda bisa mengunduh file yang sudah bersih dengan menjalankan perintah di bawah ini di sel baru:\")\n",
        "    print(f\"files.download('{cleaned_file_path}')\")\n",
        "except ImportError:\n",
        "    print(\"\\nSelesai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zAdA6rb5Wst",
        "outputId": "06fc6b0b-6a1d-4157-a583-4136244ac7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anda bisa mengunduh file yang sudah bersih dengan menjalankan perintah di bawah ini di sel baru:\n",
            "files.download('cleaned_reddit_personality.csv')\n"
          ]
        }
      ]
    }
  ]
}